# 代码手撕笔记

- Transformer层
    - 注解
        
        手敲模型的代码实现只包含前向传播部分，验证模型能否正常运行，不涉及训练过程
        
        同时对向量、矩阵的维度变化分析，举一个具体向量的例子从头贯穿到尾
        
        ![image.png](image.png)
        
    - 一、Embedding层
        
        第一次手撕这个花了两个小时，首先是维度变化不熟悉，类也不熟悉，很多细节，纯手敲也有拼写错等
        
        1.输入X：(batch_size,seq_len)  
        
        例如 形状(2,2)
        
        torch.tensor([
        [0, 1],
        [2, 3]])
        
        a.输入的值为词典整数索引序列，是在模型训练之前通过文本预处理步骤完成的，属于自然语言处理（NLP）流程中的 “词汇映射” 环节
        
        b.每句seq_len长度相同是长度对齐，通常通过填充（Padding） 和截断（Truncation）（这里没调用输入X，最后才调用，但是按流程来写）
        
        2.嵌入矩阵embedding_matrix：(vocab_size,d_model)
        
        例如 形状(4,3)
        
        torch.tensor([
        [0.00, 0.00, 0.00],  # 索引0: <pad>
        [0.82, 0.15, 0.36],  # 索引1: 猫
        
        [0.7, 0.2, 0.4],  # 索引2: 狗     
        
        [0.6, 0.3, 0.5]])  # 索引3: 鱼 
        
        a.嵌入矩阵的vocab_size参数用来指定这个词典的大小，每一个词对应一个嵌入向量
        
        b.d_model是每个词嵌入的维度（将词映射到 d_model 维的向量空间）
        
        3.输出：(batch_size, seq_len, d_model)
        
        a.原输入X索引替换为对应嵌入的d_model维度的向量
        如X[0][0]=1→embedding_matrix[1]=[0.82, 0.15, 0.36],以此按最后一个轴（-1）拼接，最后一个位置的轴，可以按照张量结构里最里面一层理解
        
        (batch_size,seq_len) →(batch_size,seq_len,d_model)  
        
        例如形状(2,2,3)
        
        torch.tensor([
        [[0.00, 0.00, 0.00],  # 第1个样本第1个词（索引0）
        [0.82, 0.15, 0.36]],   # 第1个样本第2个词（索引1）
        [[0.70, 0.20, 0.40],  # 第2个样本第1个词（索引2）
        [0.60, 0.30, 0.50]  ]]) # 第2个样本第2个词（索引3）
        
        ```python
        class Embedding(nn.Module):
        	def __init__(self,vocab_size,d_model):
        		super().__init__()#必须先调用父类的方法再初始化子类自己的属性
        		self.embedding=nn.Embedding(vocab_size,d_model) # 定义词嵌入层
        		self.d_model=d_model #方便查看维度
        		
        	def forward(self,x):
        		#将输入序列x映射为词嵌入向量
        		return self.embedding(x) #等价于self.embedding.forward(x)
        ```
        
        另外，还有一种直接继承nn.Embedding的简便形式
        
        ```python
        class Embedding(nn.Embedding):
            def __init__(self,vocab_size,d_model):
                super().__init__(vocab_size,d_model,padding_idx=1)
        ```
        
    - 二、PositionalEncoding层
        
        1.输入X（batch_size,seq_len,d_model）
        
        例如形状(2,2,3)承接上文
        
        torch.tensor([
        [[0.00, 0.00, 0.00],  
        [0.82, 0.15, 0.36]],   
        [[0.70, 0.20, 0.40], 
        [0.60, 0.30, 0.50]  ]]) 
        
        2.position
        
         [max_len,] →  [max_len, 1]（即 [5000, 1]）
        
        3div_term
        
        [d_model/2]→  [1,d_model/2]（也可以是[d_model/2]不变）
        
        4.相乘
        
        [max_len, d-model]
        
        ![image.png](cabc1c59-ed37-45b7-bdaa-7675e803d3ac.png)
        
        具体计算又做了个对数变换
        
        ![image.png](image%201.png)
        
        5.位置编码矩阵
        
        初始形状：[max_len, d_model] 
        增加批次维度后：pe.unsqueeze(0) → [1, max_len, d_model]，并注册为缓冲区，不参与梯度更新
        
        位置编码切片 [1, max_len, d_model] →[1, seq_len, d_model] 
        
        6.输出X（batch_size,seq_len,d_model）不变
        
        例如tensor.torch[
        [[0.00, 1.00, 0.00], [1.6615, 0.6903, 1.2015]],
        [[0.70, 1.20, 0.40], [1.4415, 0.8403, 1.3415]]
        ]
        
        由于广播机制（batch_size,seq_len,d_model）和（1, seq_len, d_model）相加
        
        （1, seq_len, d_model）复制batch_size份，最终形状不变
        
        ```python
        class PositionalEncoding(nn.Module):
            def __init__(self,d_model,dropout,max_len=5000):
                #有默认值的参数必须放在没有默认值的参数后面，否则会报错
                super().__init__()
                assert d_model%2==0 #必须为偶数
                self.dropout=nn.Dropout(dropout)
                self.max_len=max_len
                # 1. 创建位置编码矩阵容器
                pe=torch.zeros(max_len,d_model) # 形状：[max_len, d_model]
        				# 2. 生成位置索引向量
                position=torch.arange(0,max_len).unsqueeze(1)  #形状：[max_len, 1]，
                # 3. 计算频率除数项
                div_term=torch.exp(torch.arange(0,d_model,2).float()*(torch.log(torch.tensor(10000.0)))/d_model).unsqueeze(0)  #形状：[1,d_model/2]，也可以不扩展直接[d_model/2]利用广播机制
                # 4. 应用正弦和余弦函数生成位置编码
                pe[:,::2]=torch.sin(position*div_term)  #偶数位置使用sin
                pe[:,1::2]=torch.cos(position*div_term)  #奇数位置使用cos
                pe=pe.unsqueeze(0)  # 扩展维度，形状：[1, max_len, d_model]
        
                self.register_buffer('pe', pe)  #将pe注册为buffer，这样在调用model.to(device)时，pe会自动转移到对应设备上，包含在模型的状态字典 state_dict中，但不会被优化器更新
                #pe.requires_grad=False 也可以表示不需要计算梯度
        
            def forward(self,x):
        		    #位置编码相加
                x=x+self.pe[:,:x.size(1),:]  #原本pe的第1维是max_len，这里只截取实际长度，形状：[batch_size, seq_len, d_model]
                #也可写作x=x+self.pe[:,:x.size(1)]，Pytorch切片操作默认保留未指定维度的全部元素
                return self.dropout(x)
        
        ```
        
    - 三、MultiHeadAttention层
        
        **输入X**
        
        1.输入X[batch_size, s_seq_len, d_model] 
        
        例如torch[
        [[0.00, 1.00, 0.00], [1.6615, 0.6903, 1.2015]],
        [[0.70, 1.20, 0.40], [1.4415, 0.8403, 1.3415]]]
        
        **输入张量形状**
        
        ![image.png](image%202.png)
        
        2.query: [batch_size, s_seq_len, d_model]- 查询序列
        key: [batch_size, t_seq_len, d_model]- 键序列
        value: [batch_size, t_seq_len, d_model]- 值序列
        
        （这里没训练，具体训练要根据任务目标计算损失函数优化）
        3.mask: [batch_size, s_seq_len, t_seq_len]或广播兼容的形状 - 注意力掩码
        **线性变换后KQV**
        4.Q/K/V 线性变换后: [batch_size, seq_len, d_model]- 保持原始形状
        **多头拆分后KQV**
        
        关于KQV交换维度的原因：保证内存访问连续，交换后[batch_size, num_heads, seq_len, d_model] ，头 1 的所有seq_len个位置的d_model数据在内存中连续排列，头 2 的数据紧随其后，以此类推；
        
        同时对于batch_size，所有头的计算都是 “批处理” 的—— 每个头需要同时处理批次中的所有样本，并行计算
        
        - 内存机制
            
            PyTorch 默认用 “行优先” 存储，就是让最后一个维度的元素 “挨得最近”，像填表格一样，先把一行填满（最后一维），再换下一行（倒数第二维），直到填满整个表格
            
            ![image.png](image%203.png)
            
            如果连续，计算更快
            
        
        5.Q 重塑后: [batch_size, num_heads, s_seq_len, head_dim]
        K 重塑后: [batch_size, num_heads, t_seq_len, head_dim]
        V 重塑后: [batch_size, num_heads, t_seq_len, head_dim]
        其中 head_dim = d_model // num_heads
        **注意力计算过程**
        6.K 转置后: [batch_size, num_heads, head_dim, t_seq_len]
        7.注意力分数 (scores): [batch_size, num_heads, s_seq_len, t_seq_len]
        
        ![image.png](image%204.png)
        
        关于self.scale点积缩放
        
        ![image.png](image%205.png)
        
        8.注意力权重: [batch_size, num_heads, s_seq_len, t_seq_len]
        
        ![image.png](image%206.png)
        
        对 dim=-1（即最后一维 k_seq_len）应用 softmax，会让每个 query 对应的所有 key 的注意力分数之和为 1，满足 “权重归一化” 的要求。
        例如，对于某个 batch、某个头、某个 query 来说，它对应的 scores 是一个长度为 t_seq_len 的向量，softmax(dim=-1) 会将这个向量归一化为概率分布（和为 1），每个元素表示该 query 对某个 key 的注意力权重。
        如果对 dim=2（q_seq_len）归一化，会导致不同 query 之间的分数被归一化，这不符合逻辑（每个 query 的注意力权重应独立计算）；
        如果对 dim=0 或 dim=1 归一化，会跨 batch 或跨头混合分数，显然不合理
        
        **上下文计算后的形状**
        9.加权求和结果: [batch_size, num_heads, s_seq_len, head_dim]
        
        ![image.png](image%207.png)
        
        10.多头拼接后: [batch_size, s_seq_len, d_model]
        **最终输出形状**
        11.最终输出: [batch_size, s_seq_len, d_model]
        
        ```python
        class MultiHeadAttention(nn.Module):
            def __init__(self,d_model,num_heads,dropout):
                super().__init__()
                assert d_model%num_heads==0 #保证能拆分成整数个头
        
                self.key=nn.Linear(d_model,d_model) #形状都是[batch_size, seq_len, d_model]
                self.query=nn.Linear(d_model,d_model) 
                self.value=nn.Linear(d_model,d_model) 
                self.proj=nn.Linear(d_model,d_model) 
        
                self.d_model=d_model
                self.num_heads=num_heads
                self.head_dim=d_model//num_heads
        
                self.dropout=nn.Dropout(dropout)
                self.scale=torch.sqrt(torch.tensor(self.head_dim)) #缩放因子
                
            def forward(self,query,key,value,mask=None):
                batch_size,s_seq_len,d_model=query.shape #Source Sequence Length（源序列长度），指的是query序列的长度
                batch_size,t_seq_len,d_model=value.shape #Target Sequence Length（目标序列长度），指的是key和value序列的长度
        
                #1.输入线性变换
                #维度：[batch_size, num_heads, s_seq_len, head_dim]
                Q=self.query(query).view(batch_size,s_seq_len,self.num_heads,self.head_dim).permute(0,2,1,3) 
                K=self.key(key).view(batch_size,t_seq_len,self.num_heads,self.head_dim).permute(0,2,1,3) 
                V=self.value(value).view(batch_size,t_seq_len,self.num_heads,self.head_dim).permute(0,2,1,3) 
        
                #2.注意力分数计算（缩放点积注意力）
                #Q维度：[batch_size, num_heads, s_seq_len, head_dim]
                #K.transpose(-2, -1)：交换最后两个维度，K变为[batch_size, num_heads,head_dim, t_seq_len]
                #矩阵乘法（每个位置(i,j)表示第i个query与第j个key的相似度）
                scores=torch.matmul(Q,K.transpose(-2,-1))/self.scale #形状[batch_size,num_heads,s_seq_len,t_seq_len]
                
                #3.掩码处理
                if mask is not None: #如果存在掩码，则将掩码应用到注意力分数上
                    scores=scores.masked_fill(mask==0, float('-inf')) #将掩码位置的分数设为一个很小的值，防止其在softmax中有较大权重
                
                #4.Softmax权重计算
                attention_weights=torch.softmax(scores,dim=-1)
                
                #5.Dropout正则化
                attention_weights=self.dropout(attention_weights)
                
                #6.加权求和
                #attention_weights：[batch_size, num_heads, s_seq_len, t_seq_len]
                #V：[batch_size, num_heads, t_seq_len, head_dim]
                #矩阵乘法后：[batch_size, num_heads, s_seq_len, head_dim]
                context=torch.matmul(attention_weights,V)  #形状[batch_size,num_heads,s_seq_len,head_dim]
                
                #7.多头拼接
                #重塑回原始形状: [batch_size, s_seq_len, d_model]
                context=context.permute(0,2,1,3).contiguous().view(batch_size,s_seq_len,self.d_model)
        
                #8.最终投影
                output=self.proj(context) #形状[batch_size,seq_len,d_model]
                return output
        ```
        
    - 四、LayerNorm层
        
        输入X和输出X’维度不变，都是[batch_size, s_seq_len, d_model]
        
        输入X和输出X’维度不变，都是[batch_size, s_seq_len, d_model]
        
        关于均值方差的keepdim=True很重要，需要加上，不写就是默认False，unbiased=False有偏估计，unbiased=True无偏估计
        
        ![image.png](image%208.png)
        
        ```python
        class LayerNorm(nn.Module):
            def __init__(self,d_model,eps=1e-10):
                super().__init__()
                self.gamma=nn.Parameter(torch.ones(d_model)) # 缩放参数
                self.beta=nn.Parameter(torch.zeros(d_model)) # 平移参数
                self.eps=eps # 防止除零的小常数
                
            def forward(self,x):
                #1.计算均值和方差
                mean=x.mean(-1,keepdim=True)
                var=x.var(-1,unbiased=False,keepdim=True)
        
                #2.归一化计算
                out=(x-mean)/torch.sqrt(var+self.eps)
        
                #3.缩放和平移
                out=self.gamma*out+self.beta
                return out
        ```
        
        - LN和BN区别
            
            关于归一化，基础的有两种：LN和BN；BN是按照不同channel归一化（CV常用），LN是按照不同个体（NLP常用）归一化
            
            LN是计算同一个token所有特征维度的值进行归一化，BN是所有token的同一个位置的特征维度的值进行归一化
            
            比如3个班，5个学生，3科语数英成绩
            
            LN就是比如 1 班 1 号学生的语数英总分求均值，算标准差，之后归一化
            
            BN就是比如3个班所有15个学生的语文成绩求均值，算标准差，之后归一化
            
        - 使用LN的原因
            
            1.Transformer的输入是离散token序列，模型需要关注token之间的关联而非像 CNN中局部特征图的跨样本统计规律，使用LN就能保留单个token内部的特征尺度关系
            
            （Token based&feature map based）
            
            2.只要权重标准差稳定，每一层输出的值就能保持稳定
            
            ![image.png](image%209.png)
            
    - 五、ResidualConnection层
        
        输入和输出维度不变，都是[batch_size, s_seq_len, d_model]
        
        关于这里的残差连接使用了pre-ln现在更常用（原论文是post-ln）
        
        ```python
        class ResidualConnection(nn.Module):
            def __init__(self,d_model,drop_prob):
                super().__init__()
                self.norm=LayerNorm(d_model)  #用自己定义的LayerNorm，也可以用nn.LayerNorm
                self.dropout=nn.Dropout(drop_prob)
        
            def forward(self, x, sublayer):
                # Pre-LN: 先进行LayerNorm，再传入子层，然后dropout和残差连接
                # sublayer 是一个可调用对象（如MultiHeadAttention实例）
                return x + self.dropout(sublayer(self.norm(x)))
        ```
        
    - 六、PositionwiseFeedForward层
        
        1.输入(batch_size, seq_len, d_model)
        2.中间(batch_size, seq_len, d_model)→ (batch_size, seq_len, hidden) 
        
        hidden通常比d_model大，如512→2048
        3.输出恢复(batch_size, seq_len, d_model)
        
        注意，这里用的是现在更常用的激活函数（原论文是ReLU）
        
        ```python
        class PositionwiseFeedForward(nn.Module):
            def __init__(self,d_model,hidden,dropout=0.1):
                super().__init__()
                self.fc1=nn.Linear(d_model,hidden)
                self.fc2=nn.Linear(hidden,d_model)
                self.dropout=nn.Dropout(dropout)
            # 输入 → Linear(d_model→hidden) → ReLU → Dropout → Linear(hidden→d_model) → 输出
            def forward(self,x):
                x=self.fc1(x)    # 扩展维度
                x=F.gelu(x)      # 非线性激活
                x=self.dropout(x) # 随机失活
                x=self.fc2(x)    # 恢复维度
                return x
        ```
        
    - 七、EncoderLayer层
        
        EncoderLayer层由之前的组件构成，即：
        
        ![image.png](image%2010.png)
        
        代码实现只需连接起来即可
        
        ```python
        class EncoderLayer(nn.Module):
            def __init__(self,d_model,num_heads,hidden,dropout):
                super().__init__()
                self.self_attention=MultiHeadAttention(d_model,num_heads,dropout)
                self.feed_forward=PositionwiseFeedForward(d_model,hidden,dropout)
                self.residual1=ResidualConnection(d_model,dropout)
                self.residual2=ResidualConnection(d_model,dropout)
            def forward(self,x,mask=None):
                # 第一个子层：自注意力 + 残差连接&归一化
                x=self.residual1(x,lambda x: self.self_attention(x,x,x,mask))
                # 第二个子层：前馈网络 + 残差连接&归一化
                x=self.residual2(x,self.feed_forward)
                return x
        ```
        
    - 八、DecoderLayer层
        
        DecoderLayer层也由之前的组件构成，即：
        
        ![image.png](image%2011.png)
        
        - 关于自注意力和交叉注意力机制
            
            ### **自注意力机制 (Self-Attention)**
            
            - **输入来源**：查询(Q)、键(K)、值(V)都来自同一个输入序列
            - **应用场景**：编码器内部、解码器自注意力部分
            - **目的**：捕捉序列内部元素之间的关系
            
            ### **交叉注意力机制 (Cross-Attention)**
            
            - **输入来源**：查询(Q)来自一个序列，键(K)和值(V)来自另一个序列
            - **应用场景**：解码器与编码器之间的注意力
            - **目的**：建立两个不同序列之间的关联
        
        ```python
        class DecoderLayer(nn.Module):
            def __init__(self,d_model,num_heads,hidden,dropout):
                super().__init__()
                self.self_attention=MultiHeadAttention(d_model,num_heads,dropout)
                self.cross_attention=MultiHeadAttention(d_model,num_heads,dropout)
                self.feed_forward=PositionwiseFeedForward(d_model,hidden,dropout)
                self.residual1=ResidualConnection(d_model,dropout)
                self.residual2=ResidualConnection(d_model,dropout)
                self.residual3=ResidualConnection(d_model,dropout)
            def forward(self,x,encoder_output,src_mask=None,tgt_mask=None):
                # 第一个子层：掩码自注意力 + 残差连接&归一化
                x=self.residual1(x,lambda x: self.self_attention(x,x,x,tgt_mask))
                # 第二个子层：交叉注意力 + 残差连接&归一化
                x=self.residual2(x,lambda x: self.cross_attention(x,encoder_output,encoder_output,src_mask))
                # 第三个子层：前馈网络 + 残差连接&归一化
                x=self.residual3(x,self.feed_forward)
                return x
        ```
        
    - 九、Encoder层
        
        Encoder层就是N个EncoderLayer层堆叠，但为了方便表示我们把最开始的词嵌入和位置编码也放在了这里
        
        ```python
        class Encoder(nn.Module):
            def __init__(self,vocab_size,d_model,num_heads,hidden,num_layers,dropout,max_len=5000):
                super().__init__()
                self.embedding=Embedding(vocab_size,d_model)
                self.positional_encoding=PositionalEncoding(d_model,dropout,max_len)
                self.layers=nn.ModuleList([
                    EncoderLayer(d_model,num_heads,hidden,dropout)
                    for _ in range(num_layers)
                ])
                self.norm=LayerNorm(d_model)
            def forward(self,x,mask=None):
                # 词嵌入 + 位置编码
                x=self.embedding(x)
                x=self.positional_encoding(x)
                
                # 通过所有编码器层
                for layer in self.layers:
                    x=layer(x,mask)
        
                return x
        ```
        
    - 十、Decoder层
        
        同样Decoder层就是N个DecoderLayer层堆叠，但为了方便表示我们把最开始的词嵌入和位置编码也放在了这里
        
        ```python
        class Decoder(nn.Module):
            def __init__(self,vocab_size,d_model,num_heads,hidden,num_layers,dropout,max_len=5000):
                super().__init__()
                self.embedding=Embedding(vocab_size,d_model)
                self.positional_encoding=PositionalEncoding(d_model,dropout,max_len)
                self.layers=nn.ModuleList([
                    DecoderLayer(d_model,num_heads,hidden,dropout)
                    for _ in range(num_layers)
                ])
                self.norm=LayerNorm(d_model)
            def forward(self,x,encoder_output,src_mask=None,tgt_mask=None):
                # 词嵌入 + 位置编码
                x=self.embedding(x)
                x=self.positional_encoding(x)
                
                # 通过所有解码器层
                for layer in self.layers:
                    x=layer(x,encoder_output,src_mask,tgt_mask)
        
                return x
        ```
        
    - 十一、Transformer层
        
        由于位置编码放在了Encoder和Decoder层中Transformer=Encoder+Decoder+Linear+Softmax，实际过程中Transformer层在经过Linear层就好，得到原始分数logits更方便后续训练推理
        
        ```python
        class Transformer(nn.Module):
            def __init__(self,src_vocab_size,tgt_vocab_size,d_model,num_heads,hidden,
                         num_layers,dropout,max_len=5000):
                super().__init__()
                self.encoder=Encoder(src_vocab_size,d_model,num_heads,hidden,num_layers,dropout,max_len)
                self.decoder=Decoder(tgt_vocab_size,d_model,num_heads,hidden,num_layers,dropout,max_len)
                self.output_projection=nn.Linear(d_model,tgt_vocab_size)
                
            def forward(self,src,tgt,src_mask=None,tgt_mask=None):
                # 编码器前向传播
                encoder_output=self.encoder(src,src_mask)
                # 解码器前向传播
                decoder_output=self.decoder(tgt,encoder_output,src_mask,tgt_mask)
                # 输出logits（原始分数）
                output=self.output_projection(decoder_output)
                return output
        
        ```