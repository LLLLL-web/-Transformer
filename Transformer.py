import torch
from torch import nn
import torch.nn.functional as F

class Embedding(nn.Module):
	def __init__(self,vocab_size,d_model):
		super().__init__()#å¿…é¡»å…ˆè°ƒç”¨çˆ¶ç±»çš„æ–¹æ³•å†åˆå§‹åŒ–å­ç±»è‡ªå·±çš„å±æ€§
		self.embedding=nn.Embedding(vocab_size,d_model) # å®šä¹‰è¯åµŒå…¥å±‚
		self.d_model=d_model #æ–¹ä¾¿æŸ¥çœ‹ç»´åº¦		
	def forward(self,x):
		#å°†è¾“å…¥åºåˆ—xæ˜ å°„ä¸ºè¯åµŒå…¥å‘é‡
		return self.embedding(x) #ç­‰ä»·äºself.embedding.forward(x)

class PositionalEncoding(nn.Module):
    def __init__(self,d_model,dropout,max_len=5000):
        #æœ‰é»˜è®¤å€¼çš„å‚æ•°å¿…é¡»æ”¾åœ¨æ²¡æœ‰é»˜è®¤å€¼çš„å‚æ•°åé¢ï¼Œå¦åˆ™ä¼šæŠ¥é”™
        super().__init__()
        assert d_model%2==0 #å¿…é¡»ä¸ºå¶æ•°
        self.dropout=nn.Dropout(dropout)
        self.max_len=max_len
        # 1. åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µå®¹å™¨
        pe=torch.zeros(max_len,d_model) # å½¢çŠ¶ï¼š[max_len, d_model]
				# 2. ç”Ÿæˆä½ç½®ç´¢å¼•å‘é‡
        position=torch.arange(0,max_len).unsqueeze(1)  #å½¢çŠ¶ï¼š[max_len, 1]ï¼Œ
        # 3. è®¡ç®—é¢‘ç‡é™¤æ•°é¡¹
        div_term=torch.exp(torch.arange(0,d_model,2).float()*(torch.log(torch.tensor(10000.0)))/d_model).unsqueeze(0)  #å½¢çŠ¶ï¼š[1,d_model/2]ï¼Œä¹Ÿå¯ä»¥ä¸æ‰©å±•ç›´æ¥[d_model/2]åˆ©ç”¨å¹¿æ’­æœºåˆ¶
        # 4. åº”ç”¨æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ç”Ÿæˆä½ç½®ç¼–ç 
        pe[:,::2]=torch.sin(position*div_term)  #å¶æ•°ä½ç½®ä½¿ç”¨sin
        pe[:,1::2]=torch.cos(position*div_term)  #å¥‡æ•°ä½ç½®ä½¿ç”¨cos
        pe=pe.unsqueeze(0)  # æ‰©å±•ç»´åº¦ï¼Œå½¢çŠ¶ï¼š[1, max_len, d_model]

        self.register_buffer('pe', pe)  #å°†peæ³¨å†Œä¸ºbufferï¼Œè¿™æ ·åœ¨è°ƒç”¨model.to(device)æ—¶ï¼Œpeä¼šè‡ªåŠ¨è½¬ç§»åˆ°å¯¹åº”è®¾å¤‡ä¸Šï¼ŒåŒ…å«åœ¨æ¨¡å‹çš„çŠ¶æ€å­—å…¸ state_dictä¸­ï¼Œä½†ä¸ä¼šè¢«ä¼˜åŒ–å™¨æ›´æ–°
        #pe.requires_grad=False ä¹Ÿå¯ä»¥è¡¨ç¤ºä¸éœ€è¦è®¡ç®—æ¢¯åº¦
    def forward(self,x):
		    #ä½ç½®ç¼–ç ç›¸åŠ 
        x=x+self.pe[:,:x.size(1),:]  #åŸæœ¬peçš„ç¬¬1ç»´æ˜¯max_lenï¼Œè¿™é‡Œåªæˆªå–å®é™…é•¿åº¦ï¼Œå½¢çŠ¶ï¼š[batch_size, seq_len, d_model]
        #ä¹Ÿå¯å†™ä½œx=x+self.pe[:,:x.size(1)]ï¼ŒPytorchåˆ‡ç‰‡æ“ä½œé»˜è®¤ä¿ç•™æœªæŒ‡å®šç»´åº¦çš„å…¨éƒ¨å…ƒç´ 
        return self.dropout(x)

class MultiHeadAttention(nn.Module):
    def __init__(self,d_model,num_heads,dropout):
        super().__init__()
        assert d_model%num_heads==0 #ä¿è¯èƒ½æ‹†åˆ†æˆæ•´æ•°ä¸ªå¤´

        self.key=nn.Linear(d_model,d_model) #å½¢çŠ¶éƒ½æ˜¯[batch_size, seq_len, d_model]
        self.query=nn.Linear(d_model,d_model) 
        self.value=nn.Linear(d_model,d_model) 
        self.proj=nn.Linear(d_model,d_model) 

        self.d_model=d_model
        self.num_heads=num_heads
        self.head_dim=d_model//num_heads

        self.dropout=nn.Dropout(dropout)
        self.scale=torch.sqrt(torch.tensor(self.head_dim)) #ç¼©æ”¾å› å­
    def forward(self,query,key,value,mask=None):
        batch_size,s_seq_len,d_model=query.shape #Source Sequence Lengthï¼ˆæºåºåˆ—é•¿åº¦ï¼‰ï¼ŒæŒ‡çš„æ˜¯queryåºåˆ—çš„é•¿åº¦
        batch_size,t_seq_len,d_model=value.shape #Target Sequence Lengthï¼ˆç›®æ ‡åºåˆ—é•¿åº¦ï¼‰ï¼ŒæŒ‡çš„æ˜¯keyå’Œvalueåºåˆ—çš„é•¿åº¦

        #1.è¾“å…¥çº¿æ€§å˜æ¢
        #ç»´åº¦ï¼š[batch_size, num_heads, s_seq_len, head_dim]
        Q=self.query(query).view(batch_size,s_seq_len,self.num_heads,self.head_dim).permute(0,2,1,3) 
        K=self.key(key).view(batch_size,t_seq_len,self.num_heads,self.head_dim).permute(0,2,1,3) 
        V=self.value(value).view(batch_size,t_seq_len,self.num_heads,self.head_dim).permute(0,2,1,3) 

        #2.æ³¨æ„åŠ›åˆ†æ•°è®¡ç®—ï¼ˆç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼‰
        #Qç»´åº¦ï¼š[batch_size, num_heads, s_seq_len, head_dim]
        #K.transpose(-2, -1)ï¼šäº¤æ¢æœ€åä¸¤ä¸ªç»´åº¦ï¼ŒKå˜ä¸º[batch_size, num_heads,head_dim, t_seq_len]
        #çŸ©é˜µä¹˜æ³•ï¼ˆæ¯ä¸ªä½ç½®(i,j)è¡¨ç¤ºç¬¬iä¸ªqueryä¸ç¬¬jä¸ªkeyçš„ç›¸ä¼¼åº¦ï¼‰
        scores=torch.matmul(Q,K.transpose(-2,-1))/self.scale #å½¢çŠ¶[batch_size,num_heads,s_seq_len,t_seq_len]
        
        #3.æ©ç å¤„ç†
        if mask is not None: #å¦‚æœå­˜åœ¨æ©ç ï¼Œåˆ™å°†æ©ç åº”ç”¨åˆ°æ³¨æ„åŠ›åˆ†æ•°ä¸Š
            scores=scores.masked_fill(mask==0, float('-inf')) #å°†æ©ç ä½ç½®çš„åˆ†æ•°è®¾ä¸ºä¸€ä¸ªå¾ˆå°çš„å€¼ï¼Œé˜²æ­¢å…¶åœ¨softmaxä¸­æœ‰è¾ƒå¤§æƒé‡
        
        #4.Softmaxæƒé‡è®¡ç®—
        attention_weights=torch.softmax(scores,dim=-1)
        
        #5.Dropoutæ­£åˆ™åŒ–
        attention_weights=self.dropout(attention_weights)
        
        #6.åŠ æƒæ±‚å’Œ
        #attention_weightsï¼š[batch_size, num_heads, s_seq_len, t_seq_len]
        #Vï¼š[batch_size, num_heads, t_seq_len, head_dim]
        #çŸ©é˜µä¹˜æ³•åï¼š[batch_size, num_heads, s_seq_len, head_dim]
        context=torch.matmul(attention_weights,V)  #å½¢çŠ¶[batch_size,num_heads,s_seq_len,head_dim]
        
        #7.å¤šå¤´æ‹¼æ¥
        #é‡å¡‘å›åŸå§‹å½¢çŠ¶: [batch_size, s_seq_len, d_model]
        context=context.permute(0,2,1,3).contiguous().view(batch_size,s_seq_len,self.d_model)

        #8.æœ€ç»ˆæŠ•å½±
        output=self.proj(context) #å½¢çŠ¶[batch_size,seq_len,d_model]
        return output

class LayerNorm(nn.Module):
    def __init__(self,d_model,eps=1e-10):
        super().__init__()
        self.gamma=nn.Parameter(torch.ones(d_model))
        self.beta=nn.Parameter(torch.zeros(d_model))
        self.eps=eps
    def forward(self,x):
        #1.è®¡ç®—å‡å€¼å’Œæ–¹å·®
        mean=x.mean(-1,keepdim=True)
        var=x.var(-1,unbiased=False,keepdim=True)

        #2.å½’ä¸€åŒ–è®¡ç®—
        out=(x-mean)/torch.sqrt(var+self.eps)

        #3.ç¼©æ”¾å’Œå¹³ç§»
        out=self.gamma*out+self.beta
        return out

class ResidualConnection(nn.Module):
    def __init__(self,d_model,drop_prob):
        super().__init__()
        self.norm=LayerNorm(d_model)
        self.dropout=nn.Dropout(drop_prob)

    def forward(self,x,sublayer_output):
        # æ®‹å·®è¿æ¥: x + å­å±‚è¾“å‡º(ç»è¿‡dropout)ï¼Œç„¶åè¿›è¡ŒLayerNorm
        return self.norm(x+self.dropout(sublayer_output))


class PositionwiseFeedForward(nn.Module):
    def __init__(self,d_model,hidden,dropout=0.1):
        super().__init__()
        self.fc1=nn.Linear(d_model,hidden)
        self.fc2=nn.Linear(hidden,d_model)
        self.dropout=nn.Dropout(dropout)
    # è¾“å…¥ â†’ Linear(d_modelâ†’hidden) â†’ ReLU â†’ Dropout â†’ Linear(hiddenâ†’d_model) â†’ è¾“å‡º
    def forward(self,x):
        x=self.fc1(x)    # æ‰©å±•ç»´åº¦
        x=F.relu(x)      # éçº¿æ€§æ¿€æ´»
        x=self.dropout(x) # éšæœºå¤±æ´»
        x=self.fc2(x)    # æ¢å¤ç»´åº¦
        return x



def test_components():
    """æµ‹è¯•æ‰€æœ‰å®šä¹‰çš„ Transformer ç»„ä»¶ã€‚"""
    
    # å…±åŒå‚æ•°
    D_MODEL = 512
    VOCAB_SIZE = 10000
    SEQ_LEN = 20
    BATCH_SIZE = 32
    NUM_HEADS = 8
    DROPOUT_RATE = 0.1
    FFN_HIDDEN = 2048 # é€šå¸¸æ˜¯ d_model * 4

    print("--- å¼€å§‹æµ‹è¯• Transformer ç»„ä»¶ ---")

    # 1. Embedding æµ‹è¯•
    print("1. æµ‹è¯• Embedding...")
    embedding_layer = Embedding(VOCAB_SIZE, D_MODEL)
    # è¾“å…¥ï¼š[batch_size, seq_len]
    input_ids = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, SEQ_LEN))
    embedded_output = embedding_layer(input_ids)
    print(f"   è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
    print(f"   è¾“å‡ºå½¢çŠ¶: {embedded_output.shape}")
    assert embedded_output.shape == (BATCH_SIZE, SEQ_LEN, D_MODEL)
    print("   âœ… Embedding æµ‹è¯•é€šè¿‡")
    print("-" * 20)

    # 2. PositionalEncoding æµ‹è¯•
    print("2. æµ‹è¯• PositionalEncoding...")
    pe_layer = PositionalEncoding(D_MODEL, DROPOUT_RATE)
    # è¾“å…¥ï¼š[batch_size, seq_len, d_model] (å³ embedded_output)
    pe_output = pe_layer(embedded_output)
    print(f"   è¾“å…¥å½¢çŠ¶: {embedded_output.shape}")
    print(f"   è¾“å‡ºå½¢çŠ¶: {pe_output.shape}")
    assert pe_output.shape == (BATCH_SIZE, SEQ_LEN, D_MODEL)
    print("   âœ… PositionalEncoding æµ‹è¯•é€šè¿‡")
    print("-" * 20)

    # 3. MultiHeadAttention æµ‹è¯•
    print("3. æµ‹è¯• MultiHeadAttention (Self-Attention)...")
    mha_layer = MultiHeadAttention(D_MODEL, NUM_HEADS, DROPOUT_RATE)
    # Q, K, V éƒ½ä½¿ç”¨ pe_output (Self-Attention)
    qkv = pe_output
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„ Look-ahead Mask (ç”¨äºè§£ç å™¨ï¼Œä¸‹ä¸‰è§’çŸ©é˜µ)
    # å½¢çŠ¶ [1, 1, seq_len, seq_len]
    attn_mask = (torch.ones(SEQ_LEN, SEQ_LEN).tril() == 1).unsqueeze(0).unsqueeze(0)

    mha_output = mha_layer(qkv, qkv, qkv, mask=attn_mask)
    print(f"   è¾“å…¥(QKV)å½¢çŠ¶: {qkv.shape}")
    print(f"   è¾“å‡ºå½¢çŠ¶: {mha_output.shape}")
    assert mha_output.shape == (BATCH_SIZE, SEQ_LEN, D_MODEL)
    print("   âœ… MultiHeadAttention æµ‹è¯•é€šè¿‡")
    print("-" * 20)
    
    # 4. LayerNorm æµ‹è¯•
    print("4. æµ‹è¯• LayerNorm...")
    ln_layer = LayerNorm(D_MODEL)
    ln_output = ln_layer(pe_output)
    print(f"   è¾“å…¥å½¢çŠ¶: {pe_output.shape}")
    print(f"   è¾“å‡ºå½¢çŠ¶: {ln_output.shape}")
    assert ln_output.shape == (BATCH_SIZE, SEQ_LEN, D_MODEL)
    print("   âœ… LayerNorm æµ‹è¯•é€šè¿‡")
    print("-" * 20)

    # 5. PositionwiseFeedForward æµ‹è¯•
    print("5. æµ‹è¯• PositionwiseFeedForward...")
    ffn_layer = PositionwiseFeedForward(D_MODEL, FFN_HIDDEN, DROPOUT_RATE)
    ffn_output = ffn_layer(pe_output)
    print(f"   è¾“å…¥å½¢çŠ¶: {pe_output.shape}")
    print(f"   è¾“å‡ºå½¢çŠ¶: {ffn_output.shape}")
    assert ffn_output.shape == (BATCH_SIZE, SEQ_LEN, D_MODEL)
    print("   âœ… PositionwiseFeedForward æµ‹è¯•é€šè¿‡")
    print("-" * 20)

    # 6. ResidualConnection æµ‹è¯•
    print("6. æµ‹è¯• ResidualConnection...")
    res_layer = ResidualConnection(D_MODEL, DROPOUT_RATE)
    # x: pe_output (æœªç»å­å±‚å¤„ç†çš„è¾“å…¥)
    # sublayer_output: mha_output (å­å±‚çš„è¾“å‡º)
    res_output = res_layer(pe_output, mha_output)
    print(f"   è¾“å…¥(x)å½¢çŠ¶: {pe_output.shape}")
    print(f"   å­å±‚è¾“å‡ºå½¢çŠ¶: {mha_output.shape}")
    print(f"   è¾“å‡ºå½¢çŠ¶: {res_output.shape}")
    assert res_output.shape == (BATCH_SIZE, SEQ_LEN, D_MODEL)
    print("   âœ… ResidualConnection æµ‹è¯•é€šè¿‡")
    print("-" * 20)
    
    print("\nğŸ‰ æ‰€æœ‰ç»„ä»¶çš„å½¢çŠ¶æµ‹è¯•å‡é€šè¿‡!")

# æ‰§è¡Œæµ‹è¯•
if __name__ == '__main__':
    test_components()